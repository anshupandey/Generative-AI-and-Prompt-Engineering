{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-and-Prompt-Engineering/blob/main/VertexAI_Gemini_Prompt_Engineering_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rn1lLJXRfSZ"
      },
      "source": [
        "# Prompt Engineering Techniques\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --user --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"jrproject-402905\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "MODEL_ID = \"gemini-1.5-flash-preview-0514\"  # @param {type:\"string\"}\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lslYAvw37JGQ"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import GenerationConfig, GenerativeModel\n",
        "# load the model\n",
        "model = GenerativeModel(MODEL_ID, system_instruction=[ \"You are a helpful assistant.\",\"Your answer questions in a concise way\",],)\n",
        "\n",
        "# Set model parameters\n",
        "generation_config = GenerationConfig( temperature=0.9, top_k=32,)\n",
        "\n",
        "def generate_response(prompt,model=model):\n",
        "  contents = [prompt]\n",
        "  response = model.generate_content(contents, generation_config=generation_config,)\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Gemini 1.5 Flash model for these given below prompting techniques\n",
        "\n",
        "### There are different prompting techniques:\n",
        "**1) Multi-turn conversation or Chatbot with conversation history** <br>\n",
        "**2) Zero-shot Prompting** <br>\n",
        "**3) Few-shot Prompting** <br>\n",
        "**4) Chain-of-Thought Prompting** <br>\n",
        "**5) Tree-of-Thought Prompting** <br>\n",
        "**6) ReAct Prompting(Reason and Act)** <br>"
      ],
      "metadata": {
        "id": "26TpFzFpFPSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero-shot prompting**\n",
        "\n",
        "Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it."
      ],
      "metadata": {
        "id": "IV76I9qwIQeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''\n",
        "Message: Hi alex, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "'''\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGtltyWkH5Hu",
        "outputId": "8e398577-c10f-4ff0-8734-782958f49f7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appreciation and gratitude. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Few-shot prompting**\n",
        "\n",
        "Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response."
      ],
      "metadata": {
        "id": "uJz9R8erKb3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Can't wait to order pizza for dinner tonight\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "\"\"\"\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfn3Zh3HJnBQ",
        "outputId": "2e44c993-efd3-4a4c-fc7b-355b797e3a14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specifying the Output Format\n",
        "- You can also specify the format in which you want the model to respond.\n",
        "- In the example below, you are asking to \"give a one word response\"."
      ],
      "metadata": {
        "id": "J3F2X283K2nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Can't wait to order pizza for dinner tonight\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "\n",
        "Give a one word response.\n",
        "\"\"\"\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E26iKMrKYK0",
        "outputId": "a7942010-78a4-4b10-c064-31ff640f6a56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain-of-Thought Prompting**\n",
        "chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
      ],
      "metadata": {
        "id": "6z21K3uzVN8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPE0hYEUMGkX",
        "outputId": "615ead3a-7cea-4dc9-adab-49adb104b4fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is **False**. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree of Thoughts (ToT)**\n",
        "\n",
        "Tree of Thoughts (ToT) prompting is a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that serve as intermediate steps for general problem-solving with language models.\n",
        "\n",
        "\n",
        "How does it work?\n",
        "\n",
        "- ToT prompting breaks problems down into smaller parts, similar to CoT prompting, but goes further by combining this with the ability to explore multiple solution paths in parallel, forming a tree instead of a single chain. Each thought is generated or solved independently and passed to the next step, allowing the model to self-evaluate and decide whether to continue with that path or choose another."
      ],
      "metadata": {
        "id": "bQvoNRxqVo5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "Imagine 5 different experts are answering this question.\n",
        "They will brainstorm the answer step by step reasoning carefully and taking all facts into consideration\n",
        "All experts will write down 1 step of their thinking,\n",
        "then share it with the group.\n",
        "They will each critique their response, and the all the responses of others\n",
        "They will check their answer based on science and the laws of physics\n",
        "They will keep going through steps until they reach their conclusion taking into account the thoughts of the other experts\n",
        "If at any time they realise that there is a flaw in their logic they will backtrack to where that flaw occurred\n",
        "If any expert realises they're wrong at any point then they acknowledges this and start another train of thought\n",
        "Each expert will assign a likelihood of their current assertion being correct\n",
        "Continue until the experts agree on the single most likely location\n",
        "The question is.\n",
        "1. Carlos is at the swimming pool.\n",
        "2. He walks to the locker room, carrying a towel.\n",
        "3. He puts his watch in the towel and carries the towel tightly to a lounger at the poolside.\n",
        "4. At the lounger he opens and vigorously shakes the towel, then walks to the snack bar.\n",
        "5. He leaves the towel at the snack bar, then walks to the diving board.\n",
        "6. Later Carlos realises he has has lost his watch. Where is the single most likely location of the watch?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF7_WBM9MT78",
        "outputId": "7ab63ba7-81c4-4f00-b3a1-7576d8158cbd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Expert Brainstorming:\n",
            "\n",
            "**Expert 1:**  The watch was last seen in the towel. **Likelihood: 100%**\n",
            "\n",
            "**Expert 2:** The towel was shaken vigorously, so the watch likely fell out. **Likelihood: 90%**\n",
            "\n",
            "**Expert 3:** The towel was left at the snack bar. **Likelihood: 100%**\n",
            "\n",
            "**Expert 4:** Therefore, the watch is most likely at the snack bar, where the towel was last seen. **Likelihood: 95%**\n",
            "\n",
            "**Expert 5:**  However, Carlos walked to the diving board after leaving the towel.  It's possible he dropped the watch while walking. **Likelihood: 70%**\n",
            "\n",
            "**Expert 1:**  Good point. The watch could have fallen out while walking to the diving board. But the towel was shaken vigorously, making it more likely to have fallen out there. **Likelihood: 80%**\n",
            "\n",
            "**Expert 2:** We need to consider the laws of physics. A watch is relatively heavy and wouldn't easily fly out of a tightly held towel. It's more probable that it fell out during the shake.  **Likelihood: 95%**\n",
            "\n",
            "**Expert 3:** Agreed. The vigorous shaking makes it the most likely point of loss. **Likelihood: 95%**\n",
            "\n",
            "**Expert 4:**  But could he have dropped it when putting the watch in the towel in the locker room? **Likelihood: 50%**\n",
            "\n",
            "**Expert 5:**  That's a valid point, but it's less likely. The locker room was the initial point, not the point of vigorous shaking. **Likelihood: 60%**\n",
            "\n",
            "**Expert 1:**  Let's focus on the most likely event. The towel was shaken vigorously, making it the most likely location of the watch. **Likelihood: 90%**\n",
            "\n",
            "**Expert 2:**  The watch is most likely to be found near the snack bar, where the towel was left after being shaken. **Likelihood: 90%**\n",
            "\n",
            "**Expert 3:**  The snack bar is the single most likely location of the watch. **Likelihood: 95%**\n",
            "\n",
            "**Conclusion:** Based on the experts' reasoning, the single most likely location of the watch is **near the snack bar**.  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReAct Prompting**\n",
        "\n",
        "ReAct is a framework where LLMs are used to generate both reasoning traces and task-specific actions in an interleaved manner.\n",
        "\n",
        "Generating reasoning traces allow the model to induce, track, and update action plans, and even handle exceptions. The action step allows to interface with and gather information from external sources such as knowledge bases or environments."
      ],
      "metadata": {
        "id": "i3Sfro9fQ1sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Solve a question answering task with interleaving\n",
        "Thought,\n",
        "Action,\n",
        "Observation steps.\n",
        "\n",
        "Thought can reason about the current situation, and Action can be three types:\n",
        "\n",
        "Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\n",
        "\n",
        "Calculator[query], it performs mathematical calculations based on input query.\n",
        "\n",
        "Lookup[keyword], which returns the next sentence containing keyword in the current passage.\n",
        "\n",
        "Finish[answer], which returns the answer and finishes the task.\n",
        "\n",
        "Question:\n",
        "\n",
        "Who is Elon Musk?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u1q_MyZO68A",
        "outputId": "97187659-7c7c-469c-b64a-a0a859907457"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find information about Elon Musk.\n",
            "Action: Search[Elon Musk]\n",
            "Observation: Elon Reeve Musk (born June 28, 1971) is a business magnate and entrepreneur. He is the founder, CEO and Chief Engineer of SpaceX; early investor, CEO and Product Architect of Tesla, Inc.; founder of The Boring Company; and co-founder of Neuralink and OpenAI. \n",
            "Thought: I have the information about Elon Musk. \n",
            "Action: Finish[Elon Musk is a business magnate and entrepreneur. He is the founder, CEO and Chief Engineer of SpaceX; early investor, CEO and Product Architect of Tesla, Inc.; founder of The Boring Company; and co-founder of Neuralink and OpenAI.] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Chain-of-Verification Prompting**\n",
        "\n",
        "The Chain-of-Verification (CoVe) prompt engineering method aims to reduce hallucinations through a verification loop."
      ],
      "metadata": {
        "id": "dPnsQc3hpQ7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question=\"Name some athletes who were born in United states\"\n",
        "\n",
        "prompt=f'''Here is the question: {Question}.\n",
        "\n",
        "First, generate a response.\n",
        "\n",
        "Then, create and answer verification questions based on this response to check for accuracy. Think it through and make sure you are extremely accurate based on the question asked.\n",
        "\n",
        "After answering each verification question, consider these answers and revise the initial response to formulate a final, verified answer. Ensure the final response reflects the accuracy and findings from the verification process.'''\n",
        "\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9mjZtAJfDdm",
        "outputId": "d85777b5-a3a1-4c3e-e3cb-5c11e1763e9b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Initial Response:\n",
            "\n",
            "Some athletes born in the United States are Michael Jordan, Serena Williams, Tom Brady, and LeBron James.\n",
            "\n",
            "## Verification Questions:\n",
            "\n",
            "1. **Is Michael Jordan a U.S. citizen?** \n",
            "    * **Answer:** Yes.\n",
            "2. **Is Serena Williams a U.S. citizen?** \n",
            "    * **Answer:** Yes.\n",
            "3. **Is Tom Brady a U.S. citizen?** \n",
            "    * **Answer:** Yes.\n",
            "4. **Is LeBron James a U.S. citizen?** \n",
            "    * **Answer:** Yes.\n",
            "\n",
            "## Revised Response:\n",
            "\n",
            "The following athletes were born in the United States: Michael Jordan, Serena Williams, Tom Brady, and LeBron James. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Step-Back Prompting**\n",
        "\n",
        "A crucial rule to remember when prompting LLMs is to give them space to 'think'. You don’t want to overly constrain the model such that it can’t explore various solutions.\n",
        "\n",
        "Chain of thought reasoning is one way to push the model to think through the problem. A simple way to implement this type of reasoning is to add a statement like “think through this task step-by-step” at the end of your prompt."
      ],
      "metadata": {
        "id": "YEF7Xk88tIQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question=\"What is Quantum Physics?\"\n",
        "prompt=f'''Here is a question or task: {Question}\n",
        "\n",
        "Let's think step-by-step to answer this:\n",
        "\n",
        "Step 1) Abstract the key concepts and principles relevant to this question:\n",
        "\n",
        "Step 2) Use the abstractions to reason through the question:\n",
        "\n",
        "Final Answer: '''\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqmZDv7ApGI6",
        "outputId": "1d009bf3-3bef-4c9e-99ed-ff69ea7bf0f6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Step 1: Key Concepts and Principles\n",
            "\n",
            "* **Quantum Mechanics:** A fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles. \n",
            "* **Quantization:** The idea that certain physical quantities, like energy, can only exist in discrete values, rather than continuous ones.\n",
            "* **Wave-particle duality:** The concept that particles can exhibit wave-like properties and waves can exhibit particle-like properties.\n",
            "* **Uncertainty Principle:**  States that certain pairs of physical properties, like position and momentum, cannot be known with perfect accuracy simultaneously.\n",
            "* **Superposition:**  A quantum state where a system exists in multiple possible states at the same time until measured.\n",
            "* **Entanglement:** A phenomenon where two or more particles become linked, even if separated by vast distances, such that measuring the state of one instantaneously affects the state of the other.\n",
            "\n",
            "## Step 2: Reasoning\n",
            "\n",
            "Quantum physics deals with the behavior of matter and energy at the atomic and subatomic level, where classical physics breaks down. It introduces concepts like quantization, wave-particle duality, and the uncertainty principle, which are fundamentally different from our everyday experiences. This leads to strange and counterintuitive phenomena like superposition and entanglement, which have profound implications for our understanding of reality.\n",
            "\n",
            "## Final Answer:\n",
            "\n",
            "Quantum physics is a branch of physics that studies the behavior of matter and energy at the atomic and subatomic level. It introduces new concepts and principles that challenge our classical understanding of the world, leading to a deeper and more nuanced understanding of the fundamental nature of reality. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UfoRBs-os3rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thank You"
      ],
      "metadata": {
        "id": "23-8_wu6QI-j"
      }
    }
  ]
}